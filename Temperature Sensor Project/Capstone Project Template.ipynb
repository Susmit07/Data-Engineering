{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project will involve exploring and assessment of data from different data-sets, and write to file storages for later processing based on business demands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Scope \n",
    "The project will address - cleansing, processing of data from multiple data sources (mainly file-systems) and finally storing the cleansed data in file-storage (csv, parquet, json etc.). Finally we will perform minimilastic code quality checks on the dataframes.\n",
    "\n",
    "#### Data Sets used:\n",
    "The Data Sets used to complete this project:\n",
    "\n",
    "* I94 Immigration Data : Comes from the U.S. National Tourism and Trade Office and contains various statistics on international visitor arrival in USA and comes from the US National Tourism and Trade Office.\n",
    "* World Temperature Data: Comes from Kaggle and contains average weather temperatures by city.\n",
    "* U.S. City Demographic Data: Comes from OpenSoft and contains information about the demographics of all US cities such as average age, male and female population\n",
    "* Airport codes and related cities : Comes from https://datahub.io/core/airport-codes#data. Airport codes data contains information about different airports around the world.\n",
    "* Climate Change: Earth Surface Temperature Data : Comes from Kaggle. Contains temperature for earth surface temperature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os, re\n",
    "import configparser\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, when, lower, isnull, year, month, dayofmonth, hour, weekofyear, dayofweek, date_format, to_date\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the spark context.\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "                                        \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Exploration & Modeling for U.S. City Demographic Data \n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "* Data Read from CSV file\n",
    "* Process the data (convert all columns from string to integer and double type) and perform aggregation\n",
    "* Modify the column name and fill all null values with 0\n",
    "* Finally, write to a parquet folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read US Cities Demo dataset file.\n",
    "demographics_data=spark.read.csv(\"./us-cities-demographics.csv\", sep=';', header=True)\n",
    "# Total number of records. (O/P: 2891)\n",
    "demographics_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cast_column_type(df, cols):\n",
    "    \"\"\"\n",
    "    Convert the types of the columns according to the configuration supplied in the cols dictionary in the format {\"column_name\": type}\n",
    "    Args:\n",
    "    df : Spark dataframe to be processed. \n",
    "    cols (:obj:`dict`): Dictionary in the format of {\"column_name\": type} indicating what columns and types they should be converted to\n",
    "    \"\"\"\n",
    "    for k, v in cols.items():\n",
    "        if k in df.columns:\n",
    "            df = df.withColumn(k, df[k].cast(v))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert numeric columns to the proper types: Integer and Double (needed for aggregation in next step)\n",
    "int_cols = ['Count', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born']\n",
    "float_cols = ['Median Age', 'Average Household Size']\n",
    "# dict(zip(int_cols, len(int_cols)*[IntegerType()])) ==> {Count=IntegerType(), Male Population=IntegerType()}\n",
    "demographics_data = cast_column_type(demographics_data, dict(zip(int_cols, len(int_cols)*[IntegerType()])))\n",
    "# dict(zip(float_cols, len(float_cols)*[DoubleType()])) ==> {Median Age = DoubleType()}\n",
    "demographics_data = cast_column_type(demographics_data, dict(zip(float_cols, len(float_cols)*[DoubleType()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(City='Rockville', State='Maryland', State Code='MD', first(Total Population)=66998, first(Female Population)=35793, first(Median Age)=38.1, first(Number of Veterans)=1990, first(Foreign-born)=25047, first(Male Population)=31205, first(Average Household Size)=2.6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_agg = {\"Median Age\": \"first\", \"Male Population\": \"first\", \"Female Population\": \"first\", \n",
    "            \"Total Population\": \"first\", \"Number of Veterans\": \"first\", \"Foreign-born\": \"first\", \"Average Household Size\": \"first\"}\n",
    "# First aggregation - City, then State followed by State Code\n",
    "agg_df = demographics_data.groupby([\"City\", \"State\", \"State Code\"]).agg(first_agg)\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- StateCode: string (nullable = true)\n",
      " |-- TotalPopulation: integer (nullable = true)\n",
      " |-- FemalePopulation: integer (nullable = true)\n",
      " |-- MedianAge: double (nullable = true)\n",
      " |-- NumberVeterans: integer (nullable = true)\n",
      " |-- ForeignBorn: integer (nullable = true)\n",
      " |-- MalePopulation: integer (nullable = true)\n",
      " |-- AverageHouseholdSize: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename all columns having space in between\n",
    "demographics_column_renamed = agg_df.withColumnRenamed('State Code', 'StateCode')\\\n",
    "    .withColumnRenamed('State Code', 'StateCode')\\\n",
    "    .withColumnRenamed('first(Total Population)', 'TotalPopulation')\\\n",
    "    .withColumnRenamed('first(Female Population)', 'FemalePopulation')\\\n",
    "    .withColumnRenamed('first(Male Population)', 'MalePopulation')\\\n",
    "    .withColumnRenamed('first(Median Age)', 'MedianAge')\\\n",
    "    .withColumnRenamed('first(Number of Veterans)', 'NumberVeterans')\\\n",
    "    .withColumnRenamed('first(Foreign-born)', 'ForeignBorn')\\\n",
    "    .withColumnRenamed('first(Average Household Size)', 'AverageHouseholdSize')\\\n",
    "# Print out the schema.\n",
    "demographics_column_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------+---------------+----------------+---------+--------------+-----------+--------------+--------------------+\n",
      "|        City|     State|StateCode|TotalPopulation|FemalePopulation|MedianAge|NumberVeterans|ForeignBorn|MalePopulation|AverageHouseholdSize|\n",
      "+------------+----------+---------+---------------+----------------+---------+--------------+-----------+--------------+--------------------+\n",
      "|   Rockville|  Maryland|       MD|          66998|           35793|     38.1|          1990|      25047|         31205|                 2.6|\n",
      "|Delray Beach|   Florida|       FL|          66261|           34042|     47.9|          4232|      16639|         32219|                2.35|\n",
      "| Jersey City|New Jersey|       NJ|         264277|          132512|     34.3|          4374|     109186|        131765|                2.57|\n",
      "+------------+----------+---------+---------------+----------------+---------+--------------+-----------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The numeric columns list who will be replaced with 0 when the value is null.\n",
    "numeric_cols = ['TotalPopulation', 'FemalePopulation', 'MedianAge', 'NumberVeterans', 'ForeignBorn', 'MalePopulation',  'AverageHouseholdSize']\n",
    "\n",
    "# Fill the null values with 0\n",
    "demographics_column_renamed = demographics_column_renamed.fillna(0, numeric_cols)\n",
    "demographics_column_renamed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now write (and overwrite) transformed 'demographics' dataset onto parquet file\n",
    "demographics_column_renamed.write.mode('overwrite').parquet(\"us_cities_demographics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Exploration & Modeling for I94 Immigration Data \n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "* Data Read from sas_data folder (comprising of I94 Immigration parquet data in chunks)\n",
    "* Process the data (convert necessary columns from string to integer, double type and date type), needed downstream for date calculation  \n",
    "* Remove unwanted columns.\n",
    "* Finally, write to a parquet folder immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read i94 immigration dataset\n",
    "immigration=spark.read.parquet(\"sas_data\")\n",
    "immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "int_cols = ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', \n",
    "        'arrdate', 'i94mode', 'i94bir', 'i94visa', 'count', 'biryear', 'dtadfile', 'depdate']\n",
    "date_cols = ['arrdate', 'depdate']\n",
    "unused_cols = [\"entdepa\", \"entdepd\", \"matflag\", \"dtaddto\", \"admnum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert columns read as string/double to integer, same as we didi above\n",
    "immigration = cast_column_type(immigration, dict(zip(int_cols, len(int_cols)*[IntegerType()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The date format string preferred to our work here: YYYY-MM-DD\n",
    "# Considering 1970 as it is a start date foe EPOCH calendar\n",
    "date_format = \"%Y-%m-%d\"\n",
    "convert_date_udf = udf (lambda x : x if x is None else (timedelta(days=x) + datetime(1970, 1, 1)).strftime(date_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_date(df, cols):\n",
    "    \"\"\"\n",
    "    Convert dates in the SAS datatype to a date in a string format YYYY-MM-DD\n",
    "    \n",
    "    Args:\n",
    "        df (:obj:`SparkDataFrame`): Spark dataframe to be processed. \n",
    "            Represents the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "        cols (:obj:`list`): List of columns in the SAS date format to be convert\n",
    "    \"\"\"\n",
    "    for c in cols :\n",
    "        if c in df.columns :\n",
    "            df = df.withColumn(c, convert_date_udf(df[c]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=5748517, i94yr=2016, i94mon=4, i94cit=245, i94res=438, i94port='LOS', arrdate='2026-05-01', i94mode=1, i94addr='CA', depdate='2026-05-09', i94bir=40, i94visa=1, count=1, dtadfile=20160430, visapost='SYD', occup=None, entdepu=None, biryear=1976, gender='F', insnum=None, airline='QF', fltno='00011', visatype='B1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert SAS date to a meaningful string date in the format of YYYY-MM-DD\n",
    "immigration = convert_date(immigration, date_cols)\n",
    "immigration = immigration.drop(*unused_cols)\n",
    "immigration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration.write.mode(\"overwrite\").parquet('immigration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Exploration & Modeling for I94 Immigration Data (I94_SAS_Labels_Descriptions.SAS file)\n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "* Data Read from I94_SAS_Labels_Descriptions file\n",
    "* Process the data and prepare the data (below cell)\n",
    "* Parse based on the country, port, mode, address and visa-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sas_file_value_parser(sas_source_file, value, column):\n",
    "    \"\"\"Parses SAS Program file to return value as pandas dataframe\n",
    "    Args:\n",
    "        sas_source_file (str): SAS source code file.\n",
    "        value (str): sas value to extract.\n",
    "        column (list): list of 2 containing column names.\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    file_string = ''\n",
    "    with open(sas_source_file) as f:\n",
    "        #read every line from the file and store in file_string\n",
    "        file_string = f.read()\n",
    "        \n",
    "    # value can be: i94cntyl, i94prtl, etc.  \n",
    "    # Read from the sas - \"value\" pattern till \";\" (Considering a block of data)\n",
    "    file_string = file_string[ file_string.index(value) : ]\n",
    "    file_string = file_string[ : file_string.index(';') ]\n",
    "\n",
    "    # [1:] as every line has a leading space.\n",
    "    lines = file_string.split('\\n')[1:]\n",
    "    codes = []\n",
    "    values = []\n",
    "    \n",
    "    for line in lines :\n",
    "        #582 =  'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'\n",
    "        if '=' in line :\n",
    "            code, val = line.split('=')\n",
    "            code = code.strip()\n",
    "            val = val.strip()\n",
    "            # Ignore the first \"'\"            \n",
    "            if code [0] == \"'\" :\n",
    "                code = code[1:-1]\n",
    "            if val[0] == \"'\":\n",
    "                val = val[1:-1]\n",
    "            codes.append(code)\n",
    "            values.append(val)\n",
    "            \n",
    "    return pd.DataFrame(list(zip(codes, values)), columns=column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                                            country\n",
       "0  582  MEXICO Air Sea, and Not Reported (I-94, no lan...\n",
       "1  236                                        AFGHANISTAN\n",
       "2  101                                            ALBANIA\n",
       "3  316                                            ALGERIA\n",
       "4  102                                            ANDORRA"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse based on city/country codes.\n",
    "i94cit_res_df = sas_file_value_parser('I94_SAS_Labels_Descriptions.SAS', 'i94cntyl', ['code', 'country'])\n",
    "i94cit_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW, AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                          port\n",
       "0  ALC        ALCAN, AK             \n",
       "1  ANC        ANCHORAGE, AK         \n",
       "2  BAR  BAKER AAF - BAKER ISLAND, AK\n",
       "3  DAC        DALTONS CACHE, AK     \n",
       "4  PIZ    DEW STATION PT LAY DEW, AK"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse based on port codes.\n",
    "i94port_df = sas_file_value_parser('I94_SAS_Labels_Descriptions.SAS', 'i94prtl', ['code', 'port'])\n",
    "i94port_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Not reported</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code          mode\n",
       "0    1           Air\n",
       "1    2           Sea\n",
       "2    3          Land\n",
       "3    9  Not reported"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse based on modes.\n",
    "i94mode_df = sas_file_value_parser('I94_SAS_Labels_Descriptions.SAS', 'i94model', ['code', 'mode'])\n",
    "i94mode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code     address\n",
       "0   AL     ALABAMA\n",
       "1   AK      ALASKA\n",
       "2   AZ     ARIZONA\n",
       "3   AR    ARKANSAS\n",
       "4   CA  CALIFORNIA"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse based on addresses.\n",
    "i94addr_df = sas_file_value_parser('I94_SAS_Labels_Descriptions.SAS', 'i94addrl', ['code', 'address'])\n",
    "i94addr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>visa-type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Student</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code visa-type\n",
       "0    1  Business\n",
       "1    2  Pleasure\n",
       "2    3   Student"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse based on visas.\n",
    "i94visa_df = sas_file_value_parser('I94_SAS_Labels_Descriptions.SAS', 'I94VISA', ['code', 'visa-type'])\n",
    "i94visa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Exploration & Modeling for I94 Immigration Data \n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "* Data Read from sas_data folder (comprising of I94 Immigration parquet data in chunks)\n",
    "* Process the data only select necessary columns needed for calculation\n",
    "* Remove duplicates.\n",
    "* Extract month, year, day of month, week, day of year and store in a temporary view\n",
    "* Query the view and create a new column \"Quarter\", populate the column based on the month condition\n",
    "* Finally write to a parquet file, with partition arrival_year and arrival_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read i94 immigration dataset to create Date Frame\n",
    "i94_spark = spark.read.parquet(\"sas_data\")\n",
    "i94_spark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "|   438|    LOS|  20574|      1|  20582|    40|      1|    1|     F|94953870030|\n",
      "|   438|    LOS|  20574|      1|  20591|    32|      1|    1|     F|94955622830|\n",
      "|   438|    LOS|  20574|      1|  20582|    29|      1|    1|     M|94956406530|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_spark=i94_spark.select(col(\"i94res\").cast(IntegerType()),col(\"i94port\"),\n",
    "                           col(\"arrdate\").cast(IntegerType()), \\\n",
    "                           col(\"i94mode\").cast(IntegerType()),col(\"depdate\").cast(IntegerType()),\n",
    "                           col(\"i94bir\").cast(IntegerType()),col(\"i94visa\").cast(IntegerType()), \n",
    "                           col(\"count\").cast(IntegerType()), \\\n",
    "                              \"gender\",col(\"admnum\").cast(LongType()))\n",
    "i94_spark.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "|   582|    XXX|  20557|   null|  20558|    34|      2|    1|  null|91904214530|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will drop duplicate rows and save it as final dataset for i94\n",
    "i94_spark_unique=i94_spark.dropDuplicates()\n",
    "i94_spark_unique.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+------------+\n",
      "|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|arrival_date|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+------------+\n",
      "|   582|    XXX|  20557|   null|  20558|    34|      2|    1|  null|91904214530|  2026-04-14|\n",
      "|   209|    AGA|  20552|      1|   null|  null|      2|    1|     M|47842155333|  2026-04-09|\n",
      "|   209|    ATL|  20571|      1|   null|  null|      2|    1|     M|44537883633|  2026-04-28|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "# Convert SAS arrival date to datetime format\n",
    "convert_to_date = udf(lambda x: (dt.datetime(1970, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "i94_with_arrival_date = i94_spark_unique.withColumn(\"arrival_date\", convert_to_date(i94_spark_unique.arrdate))\n",
    "i94_with_arrival_date.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: integer (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- depdate: integer (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- admnum: long (nullable = true)\n",
      " |-- arrival_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_with_arrival_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "i94date= i94_with_arrival_date.withColumn('arrival_date_in_Date',F.to_date(i94_with_arrival_date.arrival_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: integer (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- depdate: integer (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- admnum: long (nullable = true)\n",
      " |-- arrival_date: string (nullable = true)\n",
      " |-- arrival_date_in_Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: integer (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- depdate: integer (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- admnum: long (nullable = true)\n",
      " |-- arrival_date: string (nullable = true)\n",
      " |-- arrival_date_in_Date: date (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- arrival_weekofyear: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract all month, year, day, week related information from the arrival_date.\n",
    "i94date = i94date.withColumn('arrival_month',month(i94date.arrival_date_in_Date))\n",
    "i94date = i94date.withColumn('arrival_year',year(i94date.arrival_date_in_Date))\n",
    "i94date = i94date.withColumn('arrival_day',dayofmonth(i94date.arrival_date_in_Date))\n",
    "i94date = i94date.withColumn('day_of_week',dayofweek(i94date.arrival_date_in_Date))\n",
    "i94date = i94date.withColumn('arrival_weekofyear',weekofyear(i94date.arrival_date_in_Date))\n",
    "i94date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrival_sasdate: integer (nullable = true)\n",
      " |-- arrival_iso_date: date (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_day: integer (nullable = true)\n",
      " |-- arrival_weekofyear: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates.\n",
    "i94date_filtered=i94date.select(col('arrdate').alias('arrival_sasdate'),col('arrival_date_in_Date').alias('arrival_iso_date'),'arrival_month','day_of_week','arrival_year','arrival_day','arrival_weekofyear').dropDuplicates()\n",
    "i94date_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create temporary sql table\n",
    "i94date_filtered.createOrReplaceTempView(\"i94date_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+-------------+-----------+------------+-----------+------------------+-----------+\n",
      "|arrival_sasdate|arrival_iso_date|arrival_month|day_of_week|arrival_year|arrival_day|arrival_weekofyear|    Quarter|\n",
      "+---------------+----------------+-------------+-----------+------------+-----------+------------------+-----------+\n",
      "|          20567|      2026-04-24|            4|          6|        2026|         24|                17|2nd Quarter|\n",
      "|          20551|      2026-04-08|            4|          4|        2026|          8|                15|2nd Quarter|\n",
      "|          20563|      2026-04-20|            4|          2|        2026|         20|                17|2nd Quarter|\n",
      "+---------------+----------------+-------------+-----------+------------+-----------+------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add quarters to i94 date dimension table\n",
    "i94date_quarter=spark.sql('''select arrival_sasdate,\n",
    "                         arrival_iso_date,\n",
    "                         arrival_month,\n",
    "                         day_of_week,\n",
    "                         arrival_year,\n",
    "                         arrival_day,\n",
    "                         arrival_weekofyear,\n",
    "                         CASE WHEN arrival_month IN (1, 2, 3) THEN '1st Quarter ' \n",
    "                                WHEN arrival_month IN (4, 5, 6) THEN '2nd Quarter' \n",
    "                                WHEN arrival_month IN (7, 8, 9) THEN '3rd Quarter' \n",
    "                                ELSE '4th Quarter' \n",
    "                         END AS Quarter from i94date_table''')\n",
    "i94date_quarter.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save i94date dimension to parquet file partitioned by year and month:\n",
    "i94date_quarter.write.mode(\"overwrite\").partitionBy(\"arrival_year\", \"arrival_month\").parquet('i94date_quarter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Exploration & Modeling for Global Temperature by City (GlobalLandTemperaturesByCity.csv file)\n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "* Data Read from GlobalLandTemperaturesByCity file\n",
    "* Process the data and only keep data related to United States\n",
    "* Store in a CSV file finally GlobalLandTempOfUS.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read another datset GlobalLandTemperaturesByCity.csv\n",
    "global_temp_csv = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = pd.read_csv(global_temp_csv)\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 687289 entries, 47555 to 8439246\n",
      "Data columns (total 7 columns):\n",
      "Date                             687289 non-null object\n",
      "AverageTemperature               661524 non-null float64\n",
      "AverageTemperatureUncertainty    661524 non-null float64\n",
      "City                             687289 non-null object\n",
      "Country                          687289 non-null object\n",
      "Latitude                         687289 non-null object\n",
      "Longitude                        687289 non-null object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 41.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show temperature details for country = United states.\n",
    "df_temp_us = df_temp[df_temp[\"Country\"] == \"United States\"]\n",
    "df_temp_us = df_temp_us.rename(columns={'dt': 'Date'})\n",
    "print(df_temp_us.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47555</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>2.101</td>\n",
       "      <td>3.217</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47556</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>6.926</td>\n",
       "      <td>2.853</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47557</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>10.767</td>\n",
       "      <td>2.395</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47558</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>17.989</td>\n",
       "      <td>2.202</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>21.809</td>\n",
       "      <td>2.036</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  AverageTemperature  AverageTemperatureUncertainty     City  \\\n",
       "47555  1820-01-01               2.101                          3.217  Abilene   \n",
       "47556  1820-02-01               6.926                          2.853  Abilene   \n",
       "47557  1820-03-01              10.767                          2.395  Abilene   \n",
       "47558  1820-04-01              17.989                          2.202  Abilene   \n",
       "47559  1820-05-01              21.809                          2.036  Abilene   \n",
       "\n",
       "             Country Latitude Longitude  \n",
       "47555  United States   32.95N   100.53W  \n",
       "47556  United States   32.95N   100.53W  \n",
       "47557  United States   32.95N   100.53W  \n",
       "47558  United States   32.95N   100.53W  \n",
       "47559  United States   32.95N   100.53W  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_us.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write the filtered data to a csv file for future use\n",
    "df_temp_us.to_csv('GlobalLandTempOfUS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Exploration & Modeling for Aiport Codes\n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "* Data Read from Aiport codes csv file\n",
    "* Process the data split the coordinates column to 2 sperate columns, latitude and longitude\n",
    "* Store in a CSV file finally filtered_location_data.csv with necessary filtered columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_code_data_spark =spark.read.format('csv').options(header='true').load('airport-codes_csv.csv')\n",
    "airport_code_data_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the co-ordinates column based on a delimter \",\"\n",
    "from pyspark.sql import functions as F\n",
    "split_coordinates_column = F.split(airport_code_data_spark['coordinates'], ',')\n",
    "airport_code_data_spark = airport_code_data_spark.withColumn('latitude', split_coordinates_column[0])\n",
    "airport_code_data_spark = airport_code_data_spark.withColumn('longitude', split_coordinates_column[1])\n",
    "airport_code_data_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport_code_data_spark.createOrReplaceTempView(\"airport_code_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+----------+------------------+---------------+--------+\n",
      "|         type|                name|iso_country|iso_region|          latitude|      longitude|gps_code|\n",
      "+-------------+--------------------+-----------+----------+------------------+---------------+--------+\n",
      "|     heliport|   Total Rf Heliport|         US|     US-PA|-74.93360137939453| 40.07080078125|     00A|\n",
      "|small_airport|Aero B Ranch Airport|         US|     US-KS|       -101.473911|      38.704022|    00AA|\n",
      "|small_airport|        Lowell Field|         US|     US-AK|    -151.695999146|    59.94919968|    00AK|\n",
      "+-------------+--------------------+-----------+----------+------------------+---------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selective column querying related to location.\n",
    "filtered_location_data = spark.sql('''select type,\n",
    "                         name,\n",
    "                         iso_country,\n",
    "                         iso_region,\n",
    "                         latitude,\n",
    "                         longitude,\n",
    "                         gps_code from airport_code_data_table''')\n",
    "filtered_location_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Partition by type and country.\n",
    "filtered_location_data.write.mode(\"overwrite\").partitionBy(\"type\", \"iso_country\").csv('filtered_location_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Quality Checks for all pre-processed data.\n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "* Create a spark temp view based on the individual data frame object\n",
    "* Query the view to check if proper count is returned.\n",
    "* Query the view to check if any null value is being present in any calculated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     596|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_column_renamed.createOrReplaceTempView(\"demographics_column_renamed_view\")\n",
    "demographics_column_renamed_check1 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM demographics_column_renamed_view\n",
    "\"\"\")\n",
    "demographics_column_renamed_check1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both parquet file and the in-memory view shall return equal counts = 596\n",
    "spark.read.parquet(\"us_cities_demographics\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if any null or empty string value is present in MalePopulation column\n",
    "demographics_column_renamed_check2 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM demographics_column_renamed_view\n",
    "    WHERE   MalePopulation IS NULL OR MalePopulation == \"\"\n",
    "\"\"\")\n",
    "demographics_column_renamed_check2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3096313|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check count of immigration view\n",
    "immigration.createOrReplaceTempView(\"immigration_view\")\n",
    "immigration_check1 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM immigration_view\n",
    "\"\"\")\n",
    "immigration_check1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both parquet file and the in-memory view shall return equal counts = 309613\n",
    "spark.read.parquet(\"immigration\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if any null or empty string value is present in arrdate column\n",
    "immigration_date_column_check2 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM immigration_view\n",
    "    WHERE arrdate IS NULL OR arrdate == \"\" \n",
    "\"\"\")\n",
    "immigration_date_column_check2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if there is any null value in the calculated column 'arrival_month'\n",
    "i94date_quarter_check1=spark.sql('''\n",
    "    SELECT  COUNT(*) from i94date_table\n",
    "    WHERE arrival_month IS NULL or arrival_month == \"\"\n",
    "''')\n",
    "immigration_date_column_check2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
